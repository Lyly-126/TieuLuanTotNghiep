import requests
import time
import psycopg2
from psycopg2.extras import execute_batch
from typing import Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


# ====================================
# SETUP SESSION V·ªöI RETRY
# ====================================
def create_session():
    """T·∫°o session v·ªõi retry logic"""
    session = requests.Session()
    retry = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session


# Thread-local storage cho session
thread_local = threading.local()


def get_session():
    if not hasattr(thread_local, 'session'):
        thread_local.session = create_session()
    return thread_local.session


# ====================================
# B∆Ø·ªöC 1: T·∫¢I DANH S√ÅCH 10K T·ª™
# ====================================
print("üì• Downloading 10,000 words list...")
# https://github.com/first20hours/google-10000-english
# url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa.txt"
# url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears.txt"
# url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears-short.txt"
# url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears-medium.txt"
# url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt"
url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt"

response = requests.get(url)

if response.status_code != 200:
    print("‚ùå Cannot download word list")
    exit()

words_to_crawl = response.text.strip().split('\n')
print(f"‚úÖ Loaded {len(words_to_crawl)} words")


# ====================================
# B∆Ø·ªöC 2: H√ÄM L·∫§Y D·ªÆ LI·ªÜU (IMPROVED)
# ====================================
def get_word_data(word: str) -> Optional[Dict]:
    """L·∫•y phonetic, POS, nghƒ©a ti·∫øng Vi·ªát v·ªõi error handling t·ªët h∆°n"""
    session = get_session()

    try:
        # 1. API Dictionary
        dict_url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
        dict_res = session.get(dict_url, timeout=15)

        if dict_res.status_code != 200:
            return None

        data = dict_res.json()
        if not data or not isinstance(data, list):
            return None

        data = data[0]

        # L·∫•y phonetic
        phonetic = data.get('phonetic', '')
        if not phonetic:
            for p in data.get('phonetics', []):
                if p.get('text'):
                    phonetic = p['text']
                    break

        # L·∫•y POS
        pos = None
        if data.get('meanings'):
            pos = data['meanings'][0].get('partOfSpeech')

        # 2. API Translation (v·ªõi delay v√† fallback)
        time.sleep(0.3)  # TƒÉng delay ƒë·ªÉ tr√°nh rate limit

        vi_meaning = ""
        try:
            # Method 1: Google Translate (unofficial)
            trans_url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=vi&dt=t&q={word}"
            trans_res = session.get(trans_url, timeout=10)

            if trans_res.status_code == 200:
                trans_data = trans_res.json()
                if trans_data and trans_data[0] and trans_data[0][0]:
                    vi_meaning = trans_data[0][0][0]
        except:
            # Fallback: L·∫•y definition ti·∫øng Anh
            if data.get('meanings') and data['meanings'][0].get('definitions'):
                vi_meaning = data['meanings'][0]['definitions'][0].get('definition', '')[:200]

        # Validate k·∫øt qu·∫£
        if not vi_meaning or vi_meaning == word:
            return None

        return {
            'word': word,
            'pos': pos,
            'phonetic': phonetic,
            'vi_meaning': vi_meaning
        }

    except requests.exceptions.Timeout:
        return None
    except requests.exceptions.RequestException:
        return None
    except (KeyError, IndexError, TypeError, ValueError):
        return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Unexpected error for '{word}': {str(e)[:50]}")
        return None


# ====================================
# B∆Ø·ªöC 3: MAP POS
# ====================================
POS_MAP = {
    "noun": "Danh t·ª´",
    "verb": "ƒê·ªông t·ª´",
    "adjective": "T√≠nh t·ª´",
    "adverb": "Tr·∫°ng t·ª´",
    "pronoun": "ƒê·∫°i t·ª´",
    "preposition": "Gi·ªõi t·ª´",
    "conjunction": "Li√™n t·ª´",
    "interjection": "Th√°n t·ª´",
    "determiner": "T·ª´ h·∫°n ƒë·ªãnh",
    "prep": "Gi·ªõi t·ª´"
}

# ====================================
# B∆Ø·ªöC 4: K·∫æT N·ªêI DB
# ====================================
conn = psycopg2.connect(
    dbname="flashcard_ai",
    user="postgres",
    password="123456",
    host="localhost",
    port=5432
)
cur = conn.cursor()

# Thread-safe counters
lock = threading.Lock()
success_count = 0
failed_count = 0
processed_count = 0
failed_words = []  # Track failed words

# Buffer ƒë·ªÉ batch insert
insert_buffer = []
BATCH_SIZE = 50  # Gi·∫£m xu·ªëng ƒë·ªÉ commit th∆∞·ªùng xuy√™n h∆°n


# ====================================
# B∆Ø·ªöC 5: H√ÄM X·ª¨ L√ù 1 T·ª™
# ====================================
def process_word(word: str, index: int, total: int):
    """Crawl v√† l∆∞u 1 t·ª´"""
    global success_count, failed_count, processed_count, insert_buffer, failed_words

    # Fetch data
    data = get_word_data(word)

    with lock:
        processed_count += 1

        if not data:
            failed_count += 1
            failed_words.append(word)
            if processed_count % 100 == 0:
                progress = processed_count / total * 100
                print(f"[{processed_count}/{total}] ({progress:.1f}%) ‚ùå Failed: {failed_count}")
            return

        # Th√™m v√†o buffer
        insert_buffer.append((
            data['word'],
            data['pos'],
            POS_MAP.get(data['pos'], ''),
            data['phonetic'],
            data['vi_meaning'],
            "api"
        ))

        success_count += 1

        # Progress update
        if processed_count % 100 == 0:
            progress = processed_count / total * 100
            success_rate = success_count / processed_count * 100
            print(f"[{processed_count}/{total}] ({progress:.1f}%) ‚úÖ {word} ‚Üí {data['vi_meaning'][:30]}")
            print(f"   üìä Success: {success_count} ({success_rate:.1f}%) | Failed: {failed_count}")

        # Batch insert khi buffer ƒë·∫ßy
        if len(insert_buffer) >= BATCH_SIZE:
            flush_buffer()


# ====================================
# B∆Ø·ªöC 6: H√ÄM BATCH INSERT
# ====================================
def flush_buffer():
    """Insert batch v√†o DB"""
    global insert_buffer

    if not insert_buffer:
        return

    try:
        execute_batch(cur, """
                           INSERT INTO dictionary
                               (word, part_of_speech, part_of_speech_vi, phonetic, meanings, source)
                           VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (word) DO NOTHING
                           """, insert_buffer)

        conn.commit()
        print(f"   üíæ Committed {len(insert_buffer)} words to DB")
        insert_buffer = []

    except Exception as e:
        print(f"   ‚ùå DB Batch Error: {e}")
        conn.rollback()
        insert_buffer = []


# ====================================
# B∆Ø·ªöC 7: CRAWL V·ªöI MULTITHREADING
# ====================================
start_time = time.time()
MAX_WORKERS = 5  # GI·∫¢M XU·ªêNG ƒë·ªÉ tr√°nh rate limit

print(f"\nüöÄ Starting multithreaded crawl with {MAX_WORKERS} workers...")
print(f"üìö Total words: {len(words_to_crawl)}")
print("=" * 70)

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    # Submit t·∫•t c·∫£ tasks
    futures = {
        executor.submit(process_word, word, i, len(words_to_crawl)): word
        for i, word in enumerate(words_to_crawl, 1)
    }

    # Ch·ªù ho√†n th√†nh
    for future in as_completed(futures):
        word = futures[future]
        try:
            future.result()
        except Exception as e:
            with lock:
                failed_count += 1
                failed_words.append(word)
                print(f"‚ùå Exception for '{word}': {str(e)[:50]}")

# Flush buffer cu·ªëi c√πng
flush_buffer()

cur.close()
conn.close()

# ====================================
# TH·ªêNG K√ä
# ====================================
total_time = time.time() - start_time
success_rate = success_count / len(words_to_crawl) * 100

print("\n" + "=" * 70)
print("üéâ CRAWL COMPLETED!")
print("=" * 70)
print(f"‚úÖ Success: {success_count}/{len(words_to_crawl)} ({success_rate:.1f}%)")
print(f"‚ùå Failed: {failed_count}/{len(words_to_crawl)} ({failed_count / len(words_to_crawl) * 100:.1f}%)")
print(f"‚è±Ô∏è  Total time: {total_time / 60:.1f} minutes ({total_time:.0f} seconds)")
print(f"üìä Average rate: {len(words_to_crawl) / total_time:.2f} words/sec")
print("=" * 70)

# L∆∞u failed words v√†o file ƒë·ªÉ retry sau
if failed_words:
    with open('failed_words.txt', 'w') as f:
        f.write('\n'.join(failed_words))
    print(f"\nüíæ Saved {len(failed_words)} failed words to 'failed_words.txt'")
    print("   You can retry these words later with a separate script")